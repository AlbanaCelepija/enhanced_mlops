{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov-Smirnov data drift detector on CIFAR-10\n",
    "\n",
    "### Method\n",
    "\n",
    "The drift detector applies feature-wise two-sample [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) (K-S) tests. For multivariate data, the obtained p-values for each feature are aggregated either via the [Bonferroni](https://mathworld.wolfram.com/BonferroniCorrection.html) or the [False Discovery Rate](http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_hochberg1995.pdf) (FDR) correction. The Bonferroni correction is more conservative and controls for the probability of at least one false positive. The FDR correction on the other hand allows for an expected fraction of false positives to occur.\n",
    "\n",
    "For high-dimensional data, we typically want to reduce the dimensionality before computing the feature-wise univariate K-S tests and aggregating those via the chosen correction method. Following suggestions in [Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift](https://arxiv.org/abs/1810.11953), we incorporate Untrained AutoEncoders (UAE) and black-box shift detection using the classifier's softmax outputs ([BBSDs](https://arxiv.org/abs/1802.03916)) as out-of-the box preprocessing methods and note that [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) can also be easily implemented using `scikit-learn`. Preprocessing methods which do not rely on the classifier will usually pick up drift in the input data, while BBSDs focuses on label shift. The [adversarial detector](https://arxiv.org/abs/2002.09364) which is part of the library can also be transformed into a drift detector picking up drift that reduces the performance of the classification model. We can therefore combine different preprocessing techniques to figure out if there is drift which hurts the model performance, and whether this drift can be classified as input drift or label shift.\n",
    "\n",
    "### Backend\n",
    "\n",
    "The method works with both the **PyTorch** and **TensorFlow** frameworks for the optional preprocessing step. Alibi Detect does however not install PyTorch for you. \n",
    "Check the [PyTorch docs](https://pytorch.org/) how to do this.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) consists of 60,000 32 by 32 RGB images equally distributed over 10 classes. We evaluate the drift detector on the CIFAR-10-C dataset ([Hendrycks & Dietterich, 2019](https://arxiv.org/abs/1903.12261)). The instances in\n",
    "CIFAR-10-C have been corrupted and perturbed by various types of noise, blur, brightness etc. at different levels of severity, leading to a gradual decline in the classification model performance. We also check for drift against the original test set with class imbalances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 10:12:42.254556: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-05 10:12:42.289271: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'alibi_detect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malibi_detect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KSDrift\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malibi_detect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scale_by_instance\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malibi_detect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfetching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_tf_model, fetch_detector\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'alibi_detect'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from alibi_detect.cd import KSDrift\n",
    "from alibi_detect.models.tensorflow import scale_by_instance\n",
    "from alibi_detect.utils.fetching import fetch_tf_model, fetch_detector\n",
    "from alibi_detect.saving import save_detector, load_detector\n",
    "from alibi_detect.datasets import fetch_cifar10c, corruption_types_cifar10c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Original CIFAR-10 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "y_train = y_train.astype(\"int64\").reshape(\n",
    "    -1,\n",
    ")\n",
    "y_test = y_test.astype(\"int64\").reshape(\n",
    "    -1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CIFAR-10-C, we can select from the following corruption types at 5 severity levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptions = corruption_types_cifar10c()\n",
    "print(corruptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a subset of the corruptions at corruption level 5. Each corruption type consists of perturbations on all of the original test set images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruption = [\"gaussian_noise\", \"motion_blur\", \"brightness\", \"pixelate\"]\n",
    "X_corr, y_corr = fetch_cifar10c(corruption=corruption, severity=5, return_X_y=True)\n",
    "X_corr = X_corr.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the original test set in a reference dataset and a dataset which should not be rejected under the *H<sub>0</sub>* of the K-S test. We also split the corrupted data by corruption type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n_test = X_test.shape[0]\n",
    "idx = np.random.choice(n_test, size=n_test // 2, replace=False)\n",
    "idx_h0 = np.delete(np.arange(n_test), idx, axis=0)\n",
    "X_ref, y_ref = X_test[idx], y_test[idx]\n",
    "X_h0, y_h0 = X_test[idx_h0], y_test[idx_h0]\n",
    "print(X_ref.shape, X_h0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the classes are more or less balanced\n",
    "classes, counts_ref = np.unique(y_ref, return_counts=True)\n",
    "counts_h0 = np.unique(y_h0, return_counts=True)[1]\n",
    "print(\"Class Ref H0\")\n",
    "for cl, cref, ch0 in zip(classes, counts_ref, counts_h0):\n",
    "    assert cref + ch0 == n_test // 10\n",
    "    print(\"{}     {} {}\".format(cl, cref, ch0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_corr = len(corruption)\n",
    "X_c = [X_corr[i * n_test : (i + 1) * n_test] for i in range(n_corr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the same instance for each corruption type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "n_test = X_test.shape[0]\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(X_test[i])\n",
    "plt.show()\n",
    "for _ in range(len(corruption)):\n",
    "    plt.title(corruption[_])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(X_corr[n_test * _ + i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify that the performance of a classification model on CIFAR-10 drops significantly on this perturbed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cifar10\"\n",
    "model = \"resnet32\"\n",
    "clf = fetch_tf_model(dataset, model)\n",
    "acc = clf.evaluate(scale_by_instance(X_test), y_test, batch_size=128, verbose=0)[1]\n",
    "print(\"Test set accuracy:\")\n",
    "print(\"Original {:.4f}\".format(acc))\n",
    "clf_accuracy = {\"original\": acc}\n",
    "for _ in range(len(corruption)):\n",
    "    acc = clf.evaluate(scale_by_instance(X_c[_]), y_test, batch_size=128, verbose=0)[1]\n",
    "    clf_accuracy[corruption[_]] = acc\n",
    "    print(\"{} {:.4f}\".format(corruption[_], acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the drop in performance, it is important that we detect the harmful data drift!\n",
    "\n",
    "### Detect drift\n",
    "\n",
    "First we try a drift detector using the **TensorFlow** framework for the preprocessing step. We are trying to detect data drift on high-dimensional (*32x32x3*) data using feature-wise univariate tests. It therefore makes sense to apply dimensionality reduction first. Some dimensionality reduction methods also used in [Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift](https://arxiv.org/pdf/1810.11953.pdf) are readily available: a randomly initialized encoder (**UAE** or Untrained AutoEncoder in the paper), **BBSDs** (black-box shift detection using the classifier's softmax outputs) and **PCA**.\n",
    "\n",
    "#### Random encoder\n",
    "\n",
    "First we try the randomly initialized encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, InputLayer, Reshape\n",
    "from alibi_detect.cd.tensorflow import preprocess_drift\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# define encoder\n",
    "encoding_dim = 32\n",
    "encoder_net = tf.keras.Sequential(\n",
    "    [\n",
    "        InputLayer(input_shape=(32, 32, 3)),\n",
    "        Conv2D(64, 4, strides=2, padding=\"same\", activation=tf.nn.relu),\n",
    "        Conv2D(128, 4, strides=2, padding=\"same\", activation=tf.nn.relu),\n",
    "        Conv2D(512, 4, strides=2, padding=\"same\", activation=tf.nn.relu),\n",
    "        Flatten(),\n",
    "        Dense(\n",
    "            encoding_dim,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define preprocessing function\n",
    "preprocess_fn = partial(preprocess_drift, model=encoder_net, batch_size=512)\n",
    "\n",
    "# initialise drift detector\n",
    "p_val = 0.05\n",
    "cd = KSDrift(X_ref, p_val=p_val, preprocess_fn=preprocess_fn)\n",
    "\n",
    "# we can also save/load an initialised detector\n",
    "filepath = \"my_path\"  # change to directory where detector is saved\n",
    "save_detector(cd, filepath)\n",
    "cd = load_detector(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value used by the detector for the multivariate data with *encoding_dim* features is equal to *p_val / encoding_dim* because of the [Bonferroni correction](https://mathworld.wolfram.com/BonferroniCorrection.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cd.p_val / cd.n_features == p_val / encoding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the detector thinks drift occurred on the different test sets and time the prediction calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "labels = [\"No!\", \"Yes!\"]\n",
    "\n",
    "\n",
    "def make_predictions(cd, x_h0, x_corr, corruption):\n",
    "    t = timer()\n",
    "    preds = cd.predict(x_h0)\n",
    "    dt = timer() - t\n",
    "    print(\"No corruption\")\n",
    "    print(\"Drift? {}\".format(labels[preds[\"data\"][\"is_drift\"]]))\n",
    "    print(\"Feature-wise p-values:\")\n",
    "    print(preds[\"data\"][\"p_val\"])\n",
    "    print(f\"Time (s) {dt:.3f}\")\n",
    "\n",
    "    if isinstance(x_corr, list):\n",
    "        for x, c in zip(x_corr, corruption):\n",
    "            t = timer()\n",
    "            preds = cd.predict(x)\n",
    "            dt = timer() - t\n",
    "            print(\"\")\n",
    "            print(f\"Corruption type: {c}\")\n",
    "            print(\"Drift? {}\".format(labels[preds[\"data\"][\"is_drift\"]]))\n",
    "            print(\"Feature-wise p-values:\")\n",
    "            print(preds[\"data\"][\"p_val\"])\n",
    "            print(f\"Time (s) {dt:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions(cd, X_h0, X_c, corruption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, drift was only detected on the corrupted datasets. The feature-wise p-values for each univariate K-S test per (encoded) feature before multivariate correction show that most of them are well above the $0.05$ threshold for *H0* and below for the corrupted datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BBSDs\n",
    "\n",
    "For **BBSDs**, we use the classifier's softmax outputs for black-box shift detection. This method is based on [Detecting and Correcting for Label Shift with Black Box Predictors](https://arxiv.org/abs/1802.03916). The ResNet classifier is trained on data standardised by instance so we need to rescale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scale_by_instance(X_train)\n",
    "X_test = scale_by_instance(X_test)\n",
    "X_ref = scale_by_instance(X_ref)\n",
    "X_h0 = scale_by_instance(X_h0)\n",
    "X_c = [scale_by_instance(X_c[i]) for i in range(n_corr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the detector. Here we use the output of the softmax layer to detect the drift, but other hidden layers can be extracted as well by setting *'layer'* to the index of the desired hidden layer in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd.tensorflow import HiddenOutput\n",
    "\n",
    "# define preprocessing function, we use the\n",
    "preprocess_fn = partial(\n",
    "    preprocess_drift, model=HiddenOutput(clf, layer=-1), batch_size=128\n",
    ")\n",
    "\n",
    "cd = KSDrift(X_ref, p_val=p_val, preprocess_fn=preprocess_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can see that the p-value used by the detector for the multivariate data with 10 features (number of CIFAR-10 classes) is equal to *p_val / 10* because of the [Bonferroni correction](https://mathworld.wolfram.com/BonferroniCorrection.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cd.p_val / cd.n_features == p_val / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no drift on the original held out test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions(cd, X_h0, X_c, corruption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label drift\n",
    "\n",
    "We can also check what happens when we introduce class imbalances between the reference data *X_ref* and the tested data *X_imb*. The reference data will use $75$% of the instances of the first 5 classes and only $25$% of the last 5. The data used for drift testing then uses respectively $25$% and $75$% of the test instances for the first and last 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# get index for each class in the test set\n",
    "num_classes = len(np.unique(y_test))\n",
    "idx_by_class = [np.where(y_test == c)[0] for c in range(num_classes)]\n",
    "# sample imbalanced data for different classes for X_ref and X_imb\n",
    "perc_ref = 0.75\n",
    "perc_ref_by_class = [perc_ref if c < 5 else 1 - perc_ref for c in range(num_classes)]\n",
    "n_by_class = n_test // num_classes\n",
    "X_ref = []\n",
    "X_imb, y_imb = [], []\n",
    "for _ in range(num_classes):\n",
    "    idx_class_ref = np.random.choice(\n",
    "        n_by_class, size=int(perc_ref_by_class[_] * n_by_class), replace=False\n",
    "    )\n",
    "    idx_ref = idx_by_class[_][idx_class_ref]\n",
    "    idx_class_imb = np.delete(np.arange(n_by_class), idx_class_ref, axis=0)\n",
    "    idx_imb = idx_by_class[_][idx_class_imb]\n",
    "    assert not np.array_equal(idx_ref, idx_imb)\n",
    "    X_ref.append(X_test[idx_ref])\n",
    "    X_imb.append(X_test[idx_imb])\n",
    "    y_imb.append(y_test[idx_imb])\n",
    "X_ref = np.concatenate(X_ref)\n",
    "X_imb = np.concatenate(X_imb)\n",
    "y_imb = np.concatenate(y_imb)\n",
    "print(X_ref.shape, X_imb.shape, y_imb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update reference dataset for the detector and make predictions. Note that we store the preprocessed reference data since the `preprocess_at_init` kwarg is by default True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.x_ref = cd.preprocess_fn(X_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_imb = cd.predict(X_imb)\n",
    "print(\"Drift? {}\".format(labels[preds_imb[\"data\"][\"is_drift\"]]))\n",
    "print(preds_imb[\"data\"][\"p_val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update reference data\n",
    "\n",
    "So far we have kept the reference data the same throughout the experiments. It is possible however that we want to test a new batch against the last *N* instances or against a batch of instances of fixed size where we give each instance we have seen up until now the same chance of being in the reference batch ([reservoir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling)). The `update_x_ref` argument allows you to change the reference data update rule. It is a Dict which takes as key the update rule (*'last'* for last *N* instances or *'reservoir_sampling'*) and as value the batch size *N* of the reference data. You can also save the detector after the prediction calls to save the updated reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 7500\n",
    "cd = KSDrift(\n",
    "    X_ref,\n",
    "    p_val=0.05,\n",
    "    preprocess_fn=preprocess_fn,\n",
    "    update_x_ref={\"reservoir_sampling\": N},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference data is now updated with each `predict` call. Say we start with our imbalanced reference set and make a prediction on the remaining test set data *X_imb*, then the drift detector will figure out data drift has occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_imb = cd.predict(X_imb)\n",
    "print(\"Drift? {}\".format(labels[preds_imb[\"data\"][\"is_drift\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the reference data consists of *N* instances, obtained through reservoir sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cd.x_ref.shape[0] == N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then draw a random sample from the training set and compare it with the updated reference data. This still highlights that there is data drift but will update the reference data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "perc_train = 0.5\n",
    "n_train = X_train.shape[0]\n",
    "idx_train = np.random.choice(n_train, size=int(perc_train * n_train), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = cd.predict(X_train[idx_train])\n",
    "print(\"Drift? {}\".format(labels[preds_train[\"data\"][\"is_drift\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we draw a new sample from the training set, it highlights that it is not drifting anymore against the reservoir in *X_ref*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "perc_train = 0.1\n",
    "idx_train = np.random.choice(n_train, size=int(perc_train * n_train), replace=False)\n",
    "preds_train = cd.predict(X_train[idx_train])\n",
    "print(\"Drift? {}\".format(labels[preds_train[\"data\"][\"is_drift\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate correction mechanism\n",
    "\n",
    "Instead of the Bonferroni correction for multivariate data, we can also use the less conservative [False Discovery Rate](http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_hochberg1995.pdf) (FDR) correction. See [here](https://riffyn.com/riffyn-blog/2017/10/29/false-discovery-rate) or [here](https://matthew-brett.github.io/teaching/fdr.html) for nice explanations. While the Bonferroni correction controls the probability of at least one false positive, the FDR correction controls for an expected amount of false positives. The `p_val` argument at initialisation time can be interpreted as the acceptable q-value when the FDR correction is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = KSDrift(X_ref, p_val=0.05, preprocess_fn=preprocess_fn, correction=\"fdr\")\n",
    "\n",
    "preds_imb = cd.predict(X_imb)\n",
    "print(\"Drift? {}\".format(labels[preds_imb[\"data\"][\"is_drift\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial autoencoder as a malicious drift detector\n",
    "\n",
    "We can leverage the adversarial scores obtained from an [adversarial autoencoder](https://arxiv.org/abs/2002.09364)  trained on normal data and transform it into a data drift detector. The score function of the adversarial autoencoder becomes the preprocessing function for the drift detector. The K-S test is then a simple univariate test on the adversarial scores. Importantly, an adversarial drift detector flags **malicious data drift**. We can fetch the pretrained adversarial detector from a [Google Cloud Bucket](https://console.cloud.google.com/storage/browser/seldon-models/alibi-detect/ad/cifar10/resnet32) or train one from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from alibi_detect.ad import AdversarialAE\n",
    "\n",
    "# change filepath to (absolute) directory where model is downloaded\n",
    "filepath = os.path.join(os.getcwd(), \"my_path\")\n",
    "detector_type = \"adversarial\"\n",
    "detector_name = \"base\"\n",
    "filepath = os.path.join(filepath, detector_name)\n",
    "if load_pretrained:\n",
    "    ad = fetch_detector(filepath, detector_type, dataset, detector_name, model=model)\n",
    "else:  # train detector from scratch\n",
    "    # define encoder and decoder networks\n",
    "    encoder_net = tf.keras.Sequential(\n",
    "        [\n",
    "            InputLayer(input_shape=(32, 32, 3)),\n",
    "            Conv2D(\n",
    "                32,\n",
    "                4,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_regularizer=l1(1e-5),\n",
    "            ),\n",
    "            Conv2D(\n",
    "                64,\n",
    "                4,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_regularizer=l1(1e-5),\n",
    "            ),\n",
    "            Conv2D(\n",
    "                256,\n",
    "                4,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_regularizer=l1(1e-5),\n",
    "            ),\n",
    "            Flatten(),\n",
    "            Dense(40),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    decoder_net = tf.keras.Sequential(\n",
    "        [\n",
    "            InputLayer(input_shape=(40,)),\n",
    "            Dense(4 * 4 * 128, activation=tf.nn.relu),\n",
    "            Reshape(target_shape=(4, 4, 128)),\n",
    "            Conv2DTranspose(\n",
    "                256,\n",
    "                4,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_regularizer=l1(1e-5),\n",
    "            ),\n",
    "            Conv2DTranspose(\n",
    "                64,\n",
    "                4,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_regularizer=l1(1e-5),\n",
    "            ),\n",
    "            Conv2DTranspose(\n",
    "                3,\n",
    "                4,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                activation=None,\n",
    "                kernel_regularizer=l1(1e-5),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # initialise and train detector\n",
    "    ad = AdversarialAE(encoder_net=encoder_net, decoder_net=decoder_net, model=clf)\n",
    "    ad.fit(X_train, epochs=50, batch_size=128, verbose=True)\n",
    "\n",
    "    # save the trained adversarial detector\n",
    "    save_detector(ad, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the drift detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "idx = np.random.choice(n_test, size=n_test // 2, replace=False)\n",
    "X_ref = scale_by_instance(X_test[idx])\n",
    "\n",
    "# adversarial score fn = preprocess step\n",
    "preprocess_fn = partial(ad.score, batch_size=128)\n",
    "\n",
    "cd = KSDrift(X_ref, p_val=0.05, preprocess_fn=preprocess_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make drift predictions on the original test set and corrupted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_accuracy[\"h0\"] = clf.evaluate(X_h0, y_h0, batch_size=128, verbose=0)[1]\n",
    "preds_h0 = cd.predict(X_h0)\n",
    "print(\n",
    "    \"H0: Accuracy {:.4f} -- Drift? {}\".format(\n",
    "        clf_accuracy[\"h0\"], labels[preds_h0[\"data\"][\"is_drift\"]]\n",
    "    )\n",
    ")\n",
    "clf_accuracy[\"imb\"] = clf.evaluate(X_imb, y_imb, batch_size=128, verbose=0)[1]\n",
    "preds_imb = cd.predict(X_imb)\n",
    "print(\n",
    "    \"imbalance: Accuracy {:.4f} -- Drift? {}\".format(\n",
    "        clf_accuracy[\"imb\"], labels[preds_imb[\"data\"][\"is_drift\"]]\n",
    "    )\n",
    ")\n",
    "for x, c in zip(X_c, corruption):\n",
    "    preds = cd.predict(x)\n",
    "    print(\n",
    "        \"{}: Accuracy {:.4f} -- Drift? {}\".format(\n",
    "            c, clf_accuracy[c], labels[preds[\"data\"][\"is_drift\"]]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While *X_imb* clearly exhibits input data drift due to the introduced class imbalances, it is not flagged by the adversarial drift detector since the performance of the classifier is not affected and the drift is not malicious. We can visualise this by plotting the adversarial scores together with the harmfulness of the data corruption as reflected by the drop in classifier accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_scores = {}\n",
    "score = ad.score(X_ref, batch_size=128)\n",
    "adv_scores[\"original\"] = {\"mean\": score.mean(), \"std\": score.std()}\n",
    "score = ad.score(X_h0, batch_size=128)\n",
    "adv_scores[\"h0\"] = {\"mean\": score.mean(), \"std\": score.std()}\n",
    "score = ad.score(X_imb, batch_size=128)\n",
    "adv_scores[\"imb\"] = {\"mean\": score.mean(), \"std\": score.std()}\n",
    "\n",
    "for x, c in zip(X_c, corruption):\n",
    "    score_x = ad.score(x, batch_size=128)\n",
    "    adv_scores[c] = {\"mean\": score_x.mean(), \"std\": score_x.std()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = [v[\"mean\"] for _, v in adv_scores.items()]\n",
    "stdev = [v[\"std\"] for _, v in adv_scores.items()]\n",
    "xlabels = list(adv_scores.keys())\n",
    "acc = [clf_accuracy[label] for label in xlabels]\n",
    "xticks = np.arange(len(mu))\n",
    "\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "p1 = ax.bar(xticks, mu, width, yerr=stdev, capsize=2)\n",
    "color = \"tab:red\"\n",
    "p2 = ax2.bar(xticks + width, acc, width, color=color)\n",
    "\n",
    "ax.set_title(\"Adversarial Scores and Accuracy by Corruption Type\")\n",
    "ax.set_xticks(xticks + width / 2)\n",
    "ax.set_xticklabels(xlabels, rotation=45)\n",
    "ax.legend((p1[0], p2[0]), (\"Score\", \"Accuracy\"), loc=\"upper right\", ncol=2)\n",
    "ax.set_ylabel(\"Adversarial Score\")\n",
    "\n",
    "color = \"tab:red\"\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.set_ylim((-0.26, 1.2))\n",
    "ax.set_ylim((-2, 9))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore **use the scores of the detector itself to quantify the harmfulness of the drift**! We can generalise this to all the corruptions at each severity level in CIFAR-10-C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return (y_true == y_pred).astype(int).sum() / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.utils.tensorflow import predict_batch\n",
    "\n",
    "severities = [1, 2, 3, 4, 5]\n",
    "\n",
    "score_drift = {\n",
    "    1: {\"all\": [], \"harm\": [], \"noharm\": [], \"acc\": 0},\n",
    "    2: {\"all\": [], \"harm\": [], \"noharm\": [], \"acc\": 0},\n",
    "    3: {\"all\": [], \"harm\": [], \"noharm\": [], \"acc\": 0},\n",
    "    4: {\"all\": [], \"harm\": [], \"noharm\": [], \"acc\": 0},\n",
    "    5: {\"all\": [], \"harm\": [], \"noharm\": [], \"acc\": 0},\n",
    "}\n",
    "\n",
    "y_pred = predict_batch(X_test, clf, batch_size=256).argmax(axis=1)\n",
    "score_x = ad.score(X_test, batch_size=256)\n",
    "\n",
    "for s in severities:\n",
    "    print(\"\\nSeverity: {} of {}\".format(s, len(severities)))\n",
    "\n",
    "    print(\"Loading corrupted dataset...\")\n",
    "    X_corr, y_corr = fetch_cifar10c(corruption=corruptions, severity=s, return_X_y=True)\n",
    "    X_corr = X_corr.astype(\"float32\")\n",
    "\n",
    "    print(\"Preprocess data...\")\n",
    "    X_corr = scale_by_instance(X_corr)\n",
    "\n",
    "    print(\"Make predictions on corrupted dataset...\")\n",
    "    y_pred_corr = predict_batch(X_corr, clf, batch_size=256).argmax(axis=1)\n",
    "\n",
    "    print(\"Compute adversarial scores on corrupted dataset...\")\n",
    "    score_corr = ad.score(X_corr, batch_size=256)\n",
    "\n",
    "    print(\"Get labels for malicious corruptions...\")\n",
    "    labels_corr = np.zeros(score_corr.shape[0])\n",
    "    repeat = y_corr.shape[0] // y_test.shape[0]\n",
    "    y_pred_repeat = np.tile(y_pred, (repeat,))\n",
    "    # malicious/harmful corruption: original prediction correct but\n",
    "    # prediction on corrupted data incorrect\n",
    "    idx_orig_right = np.where(y_pred_repeat == y_corr)[0]\n",
    "    idx_corr_wrong = np.where(y_pred_corr != y_corr)[0]\n",
    "    idx_harmful = np.intersect1d(idx_orig_right, idx_corr_wrong)\n",
    "    labels_corr[idx_harmful] = 1\n",
    "    labels = np.concatenate([np.zeros(X_test.shape[0]), labels_corr]).astype(int)\n",
    "    # harmless corruption: original prediction correct and prediction\n",
    "    # on corrupted data correct\n",
    "    idx_corr_right = np.where(y_pred_corr == y_corr)[0]\n",
    "    idx_harmless = np.intersect1d(idx_orig_right, idx_corr_right)\n",
    "\n",
    "    score_drift[s][\"all\"] = score_corr\n",
    "    score_drift[s][\"harm\"] = score_corr[idx_harmful]\n",
    "    score_drift[s][\"noharm\"] = score_corr[idx_harmless]\n",
    "    score_drift[s][\"acc\"] = accuracy(y_corr, y_pred_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute mean scores and standard deviations per severity level and plot the results. The plot shows the mean adversarial scores (lhs) and ResNet-32 accuracies (rhs) for increasing data corruption severity levels. Level 0 corresponds to the original test set. Harmful scores  are scores from instances which have been flipped from the correct to an incorrect prediction because of the corruption. Not harmful means that the prediction was unchanged after the corruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_noharm, std_noharm = [], []\n",
    "mu_harm, std_harm = [], []\n",
    "acc = [clf_accuracy[\"original\"]]\n",
    "for k, v in score_drift.items():\n",
    "    mu_noharm.append(v[\"noharm\"].mean())\n",
    "    std_noharm.append(v[\"noharm\"].std())\n",
    "    mu_harm.append(v[\"harm\"].mean())\n",
    "    std_harm.append(v[\"harm\"].std())\n",
    "    acc.append(v[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "\n",
    "N = 6\n",
    "ind = np.arange(N)\n",
    "width = 0.35\n",
    "\n",
    "fig_bar_cd, ax = plt.subplots()\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "p0 = ax.bar(ind[0], score_x.mean(), yerr=score_x.std(), capsize=2)\n",
    "p1 = ax.bar(ind[1:], mu_noharm, width, yerr=std_noharm, capsize=2)\n",
    "p2 = ax.bar(ind[1:] + width, mu_harm, width, yerr=std_harm, capsize=2)\n",
    "\n",
    "ax.set_title(\"Adversarial Scores and Accuracy by Corruption Severity\")\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(plot_labels)\n",
    "ax.set_ylim((-1, 6))\n",
    "ax.legend((p1[0], p2[0]), (\"Not Harmful\", \"Harmful\"), loc=\"upper right\", ncol=2)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_xlabel(\"Corruption Severity\")\n",
    "\n",
    "color = \"tab:red\"\n",
    "ax2.set_ylabel(\"Accuracy\", color=color)\n",
    "ax2.plot(acc, color=color)\n",
    "ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
